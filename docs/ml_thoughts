
http://scikit-learn.org/stable/user_guide.html

LabelSpreading -- semi supervised learning alg, take in 
many classified and unclassified examples
(more robust to noise than LabelPropagation)

use multiclass classification strategy to wrap 
LabelSpreader -- take in song input from there

compute all song attributes at once and feed ml whole 
dataset. ask for user classification of some subset 
-- how long will this take? 
-- is there any way to avoid recomputing the whole model?
   -- seems like there should be, but it's also 
      intuitively non-obvious (especially if we add
      new attributes on the fly)
-- which multiclass should we use? efficiency? 
   one-v-the-rest is "fair default"
   error correcting output codes can have multiple
   classifiers per class, correct mistakes 
   (though that does not always happen in practice, 
    simply make the same mistakes)

(most (all?) methods already support multiclass, but
use of these may improve efficiency or result)


LDA, inherently multiclass, can reduce dimensionality by
preferencing most meaningful dimensions
(QDA is quadratic instead of linear?)

Decision Trees can handle multiple types of input data 
(may make our lives easier), but are prone to overfitting
and are prone to bias -- basically, they are prone to 
being just awful, but if they work, they really work
-- Random Forests increase bias, but compensate for that
   by decreasing variance *more*


in gen:
   use fit(feature_matrix, class_codes)
   predict(feature_matrix)
where feature_matrix is n_samples x n_features

decision_function(feature_matrix) seems to give function
values related to each class (eg, likelihood val of each
class?), in form n_samples x n_classes